{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "traindata = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\traindataz.mat')#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')#E:\\00心电\\af\n",
    "data = traindata['traindataz']\n",
    "trainlable = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\trainlable')#trainlable = scio.loadmat('E:\\\\00心电\\\\af\\\\trainlable.mat') \n",
    "lable = trainlable['trainlable'][0]\n",
    "#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\2018挑战赛10s\\\\testdata201810s.mat')#traindata = scio.loadmat('E:\\\\00心电\\\\af\\\\traindata.mat')#E:\\00心电\\af\n",
    "testdata = testdata['testdata201810s'][:,0:7]\n",
    "testlable = scio.loadmat('E:\\\\.000心电\\\\实验室友情赞助文件\\\\彩云\\\\AF_fea_data\\\\10s\\\\2018挑战赛10s\\\\testtable201810s.mat')#trainlable = scio.loadmat('E:\\\\00心电\\\\af\\\\trainlable.mat') \n",
    "testlable = testlable['testtable201810s'][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testlable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66900,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lable.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66900, 7)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1804, 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.58282426,  1.09861229,  2.96649128,  1.66509091,  1.512     ,\n",
       "        1.788     ,  0.60827251])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata[2][0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.68082171,  1.02961942,  3.43967043,  0.76333333,  0.508     ,\n",
       "        1.06      ,  1.30549193])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "np.random.shuffle(data) \n",
    "np.random.seed(1)\n",
    "np.random.shuffle(lable)\n",
    "np.random.seed(1)\n",
    "np.random.shuffle(testdata) \n",
    "np.random.seed(1)\n",
    "np.random.shuffle(testlable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "input = torch.FloatTensor(data)\n",
    "label = torch.LongTensor(lable)\n",
    "input_test = torch.FloatTensor(testdata)\n",
    "label_test = torch.LongTensor(testlable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train = input[0:60001]\n",
    "label_train = label[0:60001]\n",
    "\n",
    "input_valid = input[60001:]\n",
    "label_valid = label[60001:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as Fun\n",
    "# 定义BP神经网络\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden,n_hidden2, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden2)\n",
    "        #self.hidden3 = torch.nn.Linear(n_hidden2, n_hidden3)\n",
    "        #self.hidden4 = torch.nn.Linear(n_hidden3, n_hidden4)\n",
    "        self.out = torch.nn.Linear(n_hidden2, n_output)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = Fun.relu(self.hidden(x))\n",
    "        x = Fun.relu(self.hidden2(x))\n",
    "        #x = Fun.relu(self.hidden3(x))\n",
    "        #x = Fun.relu(self.hidden4(x))\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(n_feature=7, n_hidden=10,n_hidden2=10,n_output=2)\n",
    "#optimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n",
    "optimizer = torch.optim.Adam(net.parameters())\n",
    "# SGD:随机梯度下降法\n",
    "loss_func = torch.nn.CrossEntropyLoss()\n",
    "# 设定损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7846, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "1 tensor(0.7769, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "2 tensor(0.7694, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "3 tensor(0.7622, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "4 tensor(0.7553, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "5 tensor(0.7486, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "6 tensor(0.7422, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "7 tensor(0.7359, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "8 tensor(0.7300, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "9 tensor(0.7242, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "10 tensor(0.7187, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "11 tensor(0.7133, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "12 tensor(0.7082, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "13 tensor(0.7032, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "14 tensor(0.6985, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "15 tensor(0.6939, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "16 tensor(0.6894, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "17 tensor(0.6852, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "18 tensor(0.6810, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "19 tensor(0.6770, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "20 tensor(0.6731, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "21 tensor(0.6694, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "22 tensor(0.6658, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "23 tensor(0.6623, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "24 tensor(0.6588, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "25 tensor(0.6555, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "26 tensor(0.6523, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "27 tensor(0.6491, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "28 tensor(0.6460, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "29 tensor(0.6430, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "30 tensor(0.6401, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "31 tensor(0.6372, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "32 tensor(0.6344, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "33 tensor(0.6316, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "34 tensor(0.6288, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "35 tensor(0.6261, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "36 tensor(0.6234, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "37 tensor(0.6207, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "38 tensor(0.6181, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "39 tensor(0.6154, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "40 tensor(0.6128, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "41 tensor(0.6102, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "42 tensor(0.6076, grad_fn=<NllLossBackward>) 0.4999583340277662 0.5003623713581679 0.5\n",
      "43 tensor(0.6050, grad_fn=<NllLossBackward>) 0.5012749787503542 0.501521959704305 0.5005543237250555\n",
      "44 tensor(0.6024, grad_fn=<NllLossBackward>) 0.5262578957017383 0.5257283664299174 0.5133037694013304\n",
      "45 tensor(0.5999, grad_fn=<NllLossBackward>) 0.5815903068282195 0.5819684012175678 0.5376940133037694\n",
      "46 tensor(0.5973, grad_fn=<NllLossBackward>) 0.6365227246212564 0.6344397738802725 0.552660753880266\n",
      "47 tensor(0.5947, grad_fn=<NllLossBackward>) 0.6783386943550941 0.678359182490216 0.5698447893569845\n",
      "48 tensor(0.5921, grad_fn=<NllLossBackward>) 0.7137881035316078 0.7159008551964052 0.5892461197339246\n",
      "49 tensor(0.5895, grad_fn=<NllLossBackward>) 0.7438042699288345 0.745615306566169 0.6042128603104213\n",
      "50 tensor(0.5870, grad_fn=<NllLossBackward>) 0.7682371960467326 0.7706914045513843 0.6164079822616408\n",
      "51 tensor(0.5845, grad_fn=<NllLossBackward>) 0.7862535624406259 0.7885200753732425 0.6280487804878049\n",
      "52 tensor(0.5820, grad_fn=<NllLossBackward>) 0.8004366593890102 0.8044644151326279 0.63470066518847\n",
      "53 tensor(0.5796, grad_fn=<NllLossBackward>) 0.8110531491141815 0.8153355558776634 0.6407982261640798\n",
      "54 tensor(0.5773, grad_fn=<NllLossBackward>) 0.8191696805053249 0.8234526743006233 0.6518847006651884\n",
      "55 tensor(0.5750, grad_fn=<NllLossBackward>) 0.8261195646739221 0.8296854616611103 0.6657427937915743\n",
      "56 tensor(0.5727, grad_fn=<NllLossBackward>) 0.8323194613423109 0.8357733004783302 0.6723946784922394\n",
      "57 tensor(0.5705, grad_fn=<NllLossBackward>) 0.8374693755104081 0.8414262936657486 0.6823725055432373\n",
      "58 tensor(0.5683, grad_fn=<NllLossBackward>) 0.8424359594006766 0.8472242353964342 0.6879157427937915\n",
      "59 tensor(0.5660, grad_fn=<NllLossBackward>) 0.8470192163463942 0.8496883606319756 0.6945676274944568\n",
      "60 tensor(0.5638, grad_fn=<NllLossBackward>) 0.8509191513474775 0.8538918683867227 0.7012195121951219\n",
      "61 tensor(0.5615, grad_fn=<NllLossBackward>) 0.853569107181547 0.8572256848818669 0.7045454545454546\n",
      "62 tensor(0.5592, grad_fn=<NllLossBackward>) 0.8560857319044682 0.8588201188578055 0.7128603104212861\n",
      "63 tensor(0.5569, grad_fn=<NllLossBackward>) 0.8585690238496025 0.8601246557472098 0.717849223946785\n",
      "64 tensor(0.5546, grad_fn=<NllLossBackward>) 0.8606023232946117 0.861429192636614 0.7239467849223947\n",
      "65 tensor(0.5522, grad_fn=<NllLossBackward>) 0.8622689621839636 0.862443832439484 0.7283813747228381\n",
      "66 tensor(0.5498, grad_fn=<NllLossBackward>) 0.8640355994066765 0.8631685751558197 0.7350332594235033\n",
      "67 tensor(0.5474, grad_fn=<NllLossBackward>) 0.8655189080181998 0.8643281635019568 0.741130820399113\n",
      "68 tensor(0.5450, grad_fn=<NllLossBackward>) 0.8666688888518524 0.8649079576750254 0.7466740576496674\n",
      "69 tensor(0.5426, grad_fn=<NllLossBackward>) 0.8675355410743154 0.8653428033048268 0.7522172949002217\n",
      "70 tensor(0.5401, grad_fn=<NllLossBackward>) 0.8687188546857553 0.8666473401942311 0.7555432372505543\n",
      "71 tensor(0.5376, grad_fn=<NllLossBackward>) 0.8696521724637922 0.8680968256269025 0.7610864745011087\n",
      "72 tensor(0.5351, grad_fn=<NllLossBackward>) 0.8710521491308478 0.8692564139730395 0.7660753880266076\n",
      "73 tensor(0.5326, grad_fn=<NllLossBackward>) 0.8719854669088849 0.8702710537759095 0.7688470066518847\n",
      "74 tensor(0.5300, grad_fn=<NllLossBackward>) 0.8729187846869219 0.8725902304681838 0.7738359201773836\n",
      "75 tensor(0.5275, grad_fn=<NllLossBackward>) 0.8741520974650423 0.8737498188143209 0.7760532150776053\n",
      "76 tensor(0.5249, grad_fn=<NllLossBackward>) 0.875202079965334 0.8741846644441224 0.7799334811529933\n",
      "77 tensor(0.5223, grad_fn=<NllLossBackward>) 0.8764187263545608 0.875924046963328 0.7843680709534369\n",
      "78 tensor(0.5197, grad_fn=<NllLossBackward>) 0.8775853735771071 0.8763588925931295 0.791019955654102\n",
      "79 tensor(0.5170, grad_fn=<NllLossBackward>) 0.8791686805219913 0.8780982751123351 0.7937915742793792\n",
      "80 tensor(0.5143, grad_fn=<NllLossBackward>) 0.8805853235779404 0.8795477605450065 0.799889135254989\n",
      "81 tensor(0.5116, grad_fn=<NllLossBackward>) 0.8818019699671672 0.8807073488911437 0.8076496674057649\n",
      "82 tensor(0.5089, grad_fn=<NllLossBackward>) 0.8830686155230746 0.8830265255834179 0.8137472283813747\n",
      "83 tensor(0.5061, grad_fn=<NllLossBackward>) 0.8845852569123848 0.8846209595593564 0.8181818181818182\n",
      "84 tensor(0.5033, grad_fn=<NllLossBackward>) 0.8859852335794404 0.886215393535295 0.8220620842572062\n",
      "85 tensor(0.5005, grad_fn=<NllLossBackward>) 0.8876352060798987 0.8879547760545007 0.8259423503325942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 tensor(0.4976, grad_fn=<NllLossBackward>) 0.8889518508024866 0.8899840556602406 0.8298226164079823\n",
      "87 tensor(0.4947, grad_fn=<NllLossBackward>) 0.8902351627472875 0.8909986954631106 0.835920177383592\n",
      "88 tensor(0.4918, grad_fn=<NllLossBackward>) 0.8918351360810653 0.8933178721553848 0.8409090909090909\n",
      "89 tensor(0.4889, grad_fn=<NllLossBackward>) 0.8935517741370977 0.8943325119582548 0.8475609756097561\n",
      "90 tensor(0.4859, grad_fn=<NllLossBackward>) 0.8951517474708754 0.8963617915639948 0.8536585365853658\n",
      "91 tensor(0.4829, grad_fn=<NllLossBackward>) 0.8967683871935468 0.8985360197130019 0.8586474501108647\n",
      "92 tensor(0.4798, grad_fn=<NllLossBackward>) 0.8981350310828153 0.8988259167995362 0.8608647450110865\n",
      "93 tensor(0.4767, grad_fn=<NllLossBackward>) 0.8992016799720005 0.8999855051456733 0.8658536585365854\n",
      "94 tensor(0.4735, grad_fn=<NllLossBackward>) 0.9003849935834403 0.901724887664879 0.8713968957871396\n",
      "95 tensor(0.4704, grad_fn=<NllLossBackward>) 0.9018183030282828 0.9024496303812146 0.8736141906873615\n",
      "96 tensor(0.4671, grad_fn=<NllLossBackward>) 0.9028349527507875 0.9028844760110161 0.8780487804878049\n",
      "97 tensor(0.4639, grad_fn=<NllLossBackward>) 0.9040849319178014 0.9046238585302218 0.8813747228381374\n",
      "98 tensor(0.4606, grad_fn=<NllLossBackward>) 0.9054182430292829 0.9054935497898247 0.8852549889135255\n",
      "99 tensor(0.4572, grad_fn=<NllLossBackward>) 0.9059349010849819 0.9063632410494274 0.8896895787139689\n",
      "100 tensor(0.4539, grad_fn=<NllLossBackward>) 0.9070848819186347 0.9065081895926946 0.8919068736141907\n",
      "101 tensor(0.4504, grad_fn=<NllLossBackward>) 0.9080515324744588 0.9067980866792289 0.8963414634146342\n",
      "102 tensor(0.4470, grad_fn=<NllLossBackward>) 0.909184846919218 0.9082475721119003 0.9018847006651884\n",
      "103 tensor(0.4435, grad_fn=<NllLossBackward>) 0.910118164697255 0.9086824177417017 0.9035476718403548\n",
      "104 tensor(0.4399, grad_fn=<NllLossBackward>) 0.9107681538641023 0.9099869546311059 0.9041019955654102\n",
      "105 tensor(0.4363, grad_fn=<NllLossBackward>) 0.9116014733087782 0.9108566458907088 0.9096452328159645\n",
      "106 tensor(0.4327, grad_fn=<NllLossBackward>) 0.9122847952534124 0.9108566458907088 0.9124168514412417\n",
      "107 tensor(0.4291, grad_fn=<NllLossBackward>) 0.9131347810869819 0.9105667488041745 0.9157427937915743\n",
      "108 tensor(0.4254, grad_fn=<NllLossBackward>) 0.913718104698255 0.9117263371503116 0.9185144124168514\n",
      "109 tensor(0.4217, grad_fn=<NllLossBackward>) 0.9144180930317828 0.9123061313233802 0.9190687361419069\n",
      "110 tensor(0.4180, grad_fn=<NllLossBackward>) 0.9151680805319912 0.9134657196695173 0.9223946784922394\n",
      "111 tensor(0.4143, grad_fn=<NllLossBackward>) 0.9159180680321994 0.9146253080156544 0.9251662971175166\n",
      "112 tensor(0.4105, grad_fn=<NllLossBackward>) 0.9167847202546624 0.915205102188723 0.9290465631929047\n",
      "113 tensor(0.4067, grad_fn=<NllLossBackward>) 0.9174847085881902 0.9167995361646616 0.9290465631929047\n",
      "114 tensor(0.4030, grad_fn=<NllLossBackward>) 0.9185013583106948 0.9173793303377301 0.9323725055432373\n",
      "115 tensor(0.3992, grad_fn=<NllLossBackward>) 0.9193513441442642 0.9182490215973329 0.9351441241685144\n",
      "116 tensor(0.3954, grad_fn=<NllLossBackward>) 0.9199846669222179 0.9196985070300043 0.9379157427937915\n",
      "117 tensor(0.3916, grad_fn=<NllLossBackward>) 0.9204846585890235 0.9210030439194086 0.9406873614190687\n",
      "118 tensor(0.3878, grad_fn=<NllLossBackward>) 0.9211513141447643 0.92143788954921 0.9423503325942351\n",
      "119 tensor(0.3840, grad_fn=<NllLossBackward>) 0.9215346410893152 0.9221626322655457 0.9462305986696231\n",
      "120 tensor(0.3803, grad_fn=<NllLossBackward>) 0.9219846335894402 0.9221626322655457 0.947339246119734\n",
      "121 tensor(0.3766, grad_fn=<NllLossBackward>) 0.9226679555340744 0.92143788954921 0.9495565410199557\n",
      "122 tensor(0.3728, grad_fn=<NllLossBackward>) 0.9231512808119865 0.9218727351790115 0.9512195121951219\n",
      "123 tensor(0.3692, grad_fn=<NllLossBackward>) 0.9235346077565374 0.9217277866357443 0.9545454545454546\n",
      "124 tensor(0.3655, grad_fn=<NllLossBackward>) 0.9240179330344495 0.9218727351790115 0.9567627494456763\n",
      "125 tensor(0.3619, grad_fn=<NllLossBackward>) 0.9242679288678523 0.9231772720684157 0.9573170731707317\n",
      "126 tensor(0.3583, grad_fn=<NllLossBackward>) 0.9247012549790837 0.9239020147847514 0.9584257206208425\n",
      "127 tensor(0.3548, grad_fn=<NllLossBackward>) 0.9250179163680605 0.92448180895782 0.9611973392461197\n",
      "128 tensor(0.3513, grad_fn=<NllLossBackward>) 0.9252512458125698 0.9249166545876214 0.9628603104212861\n",
      "129 tensor(0.3479, grad_fn=<NllLossBackward>) 0.9254679088681855 0.9252065516741557 0.9628603104212861\n",
      "130 tensor(0.3445, grad_fn=<NllLossBackward>) 0.9256179063682272 0.9252065516741557 0.9628603104212861\n",
      "131 tensor(0.3412, grad_fn=<NllLossBackward>) 0.9257345710904819 0.9256413973039571 0.9645232815964523\n",
      "132 tensor(0.3379, grad_fn=<NllLossBackward>) 0.92586790220163 0.9256413973039571 0.9645232815964523\n",
      "133 tensor(0.3347, grad_fn=<NllLossBackward>) 0.9258512358127364 0.9256413973039571 0.9656319290465631\n",
      "134 tensor(0.3315, grad_fn=<NllLossBackward>) 0.9258179030349494 0.9254964487606899 0.9667405764966741\n",
      "135 tensor(0.3285, grad_fn=<NllLossBackward>) 0.9257679038682689 0.9256413973039571 0.9684035476718403\n",
      "136 tensor(0.3254, grad_fn=<NllLossBackward>) 0.9258012366460558 0.9256413973039571 0.9689578713968958\n",
      "137 tensor(0.3225, grad_fn=<NllLossBackward>) 0.9258179030349494 0.9254964487606899 0.9695121951219512\n",
      "138 tensor(0.3196, grad_fn=<NllLossBackward>) 0.925901234979417 0.9253515002174229 0.9700665188470067\n",
      "139 tensor(0.3168, grad_fn=<NllLossBackward>) 0.9259345677572041 0.9253515002174229 0.970620842572062\n",
      "140 tensor(0.3141, grad_fn=<NllLossBackward>) 0.9259512341460976 0.9252065516741557 0.9695121951219512\n",
      "141 tensor(0.3114, grad_fn=<NllLossBackward>) 0.9261012316461392 0.9250616031308886 0.9695121951219512\n",
      "142 tensor(0.3088, grad_fn=<NllLossBackward>) 0.9262512291461809 0.9250616031308886 0.9695121951219512\n",
      "143 tensor(0.3063, grad_fn=<NllLossBackward>) 0.9262345627572873 0.9252065516741557 0.9695121951219512\n",
      "144 tensor(0.3038, grad_fn=<NllLossBackward>) 0.9264512258129031 0.9252065516741557 0.9700665188470067\n",
      "145 tensor(0.3014, grad_fn=<NllLossBackward>) 0.9266012233129448 0.9252065516741557 0.9700665188470067\n",
      "146 tensor(0.2990, grad_fn=<NllLossBackward>) 0.9267345544240929 0.9252065516741557 0.9700665188470067\n",
      "147 tensor(0.2967, grad_fn=<NllLossBackward>) 0.9270178830352828 0.9256413973039571 0.970620842572062\n",
      "148 tensor(0.2943, grad_fn=<NllLossBackward>) 0.9269512174797087 0.9257863458472242 0.970620842572062\n",
      "149 tensor(0.2921, grad_fn=<NllLossBackward>) 0.9269178847019216 0.9257863458472242 0.970620842572062\n",
      "150 tensor(0.2899, grad_fn=<NllLossBackward>) 0.9271178813686438 0.9259312943904914 0.970620842572062\n",
      "151 tensor(0.2877, grad_fn=<NllLossBackward>) 0.9271845469242179 0.9262211914770256 0.970620842572062\n",
      "152 tensor(0.2855, grad_fn=<NllLossBackward>) 0.9273512108131531 0.9262211914770256 0.970620842572062\n",
      "153 tensor(0.2834, grad_fn=<NllLossBackward>) 0.9276845385910235 0.9260762429337586 0.970620842572062\n",
      "154 tensor(0.2814, grad_fn=<NllLossBackward>) 0.9278178697021716 0.9262211914770256 0.970620842572062\n",
      "155 tensor(0.2794, grad_fn=<NllLossBackward>) 0.9280011999800003 0.9262211914770256 0.970620842572062\n",
      "156 tensor(0.2774, grad_fn=<NllLossBackward>) 0.9282845285911902 0.9268009856500942 0.9711751662971175\n",
      "157 tensor(0.2754, grad_fn=<NllLossBackward>) 0.9283511941467643 0.9268009856500942 0.970620842572062\n",
      "158 tensor(0.2735, grad_fn=<NllLossBackward>) 0.92856785720238 0.9270908827366285 0.970620842572062\n",
      "159 tensor(0.2717, grad_fn=<NllLossBackward>) 0.9287178547024216 0.9270908827366285 0.9717294900221729\n",
      "160 tensor(0.2698, grad_fn=<NllLossBackward>) 0.9288011866468893 0.9270908827366285 0.9717294900221729\n",
      "161 tensor(0.2680, grad_fn=<NllLossBackward>) 0.9288678522024633 0.9268009856500942 0.9722838137472284\n",
      "162 tensor(0.2663, grad_fn=<NllLossBackward>) 0.9288345194246762 0.9268009856500942 0.9722838137472284\n",
      "163 tensor(0.2645, grad_fn=<NllLossBackward>) 0.9289678505358244 0.9268009856500942 0.9722838137472284\n",
      "164 tensor(0.2629, grad_fn=<NllLossBackward>) 0.929084515258079 0.9268009856500942 0.9717294900221729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165 tensor(0.2612, grad_fn=<NllLossBackward>) 0.9291511808136531 0.9266560371068271 0.9717294900221729\n",
      "166 tensor(0.2596, grad_fn=<NllLossBackward>) 0.9291011816469725 0.9268009856500942 0.9717294900221729\n",
      "167 tensor(0.2580, grad_fn=<NllLossBackward>) 0.929051182480292 0.9268009856500942 0.9717294900221729\n",
      "168 tensor(0.2565, grad_fn=<NllLossBackward>) 0.9292178463692272 0.9269459341933614 0.9717294900221729\n",
      "169 tensor(0.2550, grad_fn=<NllLossBackward>) 0.9291845135914402 0.9270908827366285 0.9717294900221729\n",
      "170 tensor(0.2535, grad_fn=<NllLossBackward>) 0.9292511791470142 0.9270908827366285 0.9717294900221729\n",
      "171 tensor(0.2520, grad_fn=<NllLossBackward>) 0.9293011783136947 0.9269459341933614 0.9717294900221729\n",
      "172 tensor(0.2506, grad_fn=<NllLossBackward>) 0.929434509424843 0.9269459341933614 0.9717294900221729\n",
      "173 tensor(0.2493, grad_fn=<NllLossBackward>) 0.929434509424843 0.9270908827366285 0.9717294900221729\n",
      "174 tensor(0.2479, grad_fn=<NllLossBackward>) 0.9294678422026299 0.9270908827366285 0.9722838137472284\n",
      "175 tensor(0.2466, grad_fn=<NllLossBackward>) 0.9293845102581624 0.9270908827366285 0.9722838137472284\n",
      "176 tensor(0.2453, grad_fn=<NllLossBackward>) 0.9294011766470559 0.9269459341933614 0.9722838137472284\n",
      "177 tensor(0.2441, grad_fn=<NllLossBackward>) 0.9293511774803753 0.9268009856500942 0.9728381374722838\n",
      "178 tensor(0.2428, grad_fn=<NllLossBackward>) 0.9292845119248012 0.9269459341933614 0.9728381374722838\n",
      "179 tensor(0.2416, grad_fn=<NllLossBackward>) 0.9293678438692689 0.9270908827366285 0.9733924611973392\n",
      "180 tensor(0.2405, grad_fn=<NllLossBackward>) 0.9294511758137365 0.9272358312798956 0.9745011086474501\n",
      "181 tensor(0.2393, grad_fn=<NllLossBackward>) 0.9293678438692689 0.9270908827366285 0.9739467849223947\n",
      "182 tensor(0.2382, grad_fn=<NllLossBackward>) 0.9293678438692689 0.9272358312798956 0.9739467849223947\n",
      "183 tensor(0.2371, grad_fn=<NllLossBackward>) 0.9294678422026299 0.9272358312798956 0.9739467849223947\n",
      "184 tensor(0.2361, grad_fn=<NllLossBackward>) 0.929534507758204 0.9272358312798956 0.9739467849223947\n",
      "185 tensor(0.2350, grad_fn=<NllLossBackward>) 0.9296345060915652 0.9273807798231628 0.9739467849223947\n",
      "186 tensor(0.2340, grad_fn=<NllLossBackward>) 0.9296845052582456 0.9273807798231628 0.9739467849223947\n",
      "187 tensor(0.2330, grad_fn=<NllLossBackward>) 0.9296511724804587 0.9273807798231628 0.9739467849223947\n",
      "188 tensor(0.2321, grad_fn=<NllLossBackward>) 0.9298011699805003 0.9272358312798956 0.9739467849223947\n",
      "189 tensor(0.2311, grad_fn=<NllLossBackward>) 0.9298178363693939 0.9272358312798956 0.9739467849223947\n",
      "190 tensor(0.2302, grad_fn=<NllLossBackward>) 0.9297845035916068 0.9273807798231628 0.9739467849223947\n",
      "191 tensor(0.2293, grad_fn=<NllLossBackward>) 0.9298345027582874 0.9275257283664299 0.9739467849223947\n",
      "192 tensor(0.2284, grad_fn=<NllLossBackward>) 0.9299011683138614 0.9273807798231628 0.9745011086474501\n",
      "193 tensor(0.2276, grad_fn=<NllLossBackward>) 0.929984500258329 0.9270908827366285 0.9745011086474501\n",
      "194 tensor(0.2268, grad_fn=<NllLossBackward>) 0.9299678338694355 0.9269459341933614 0.9750554323725056\n",
      "195 tensor(0.2260, grad_fn=<NllLossBackward>) 0.9299678338694355 0.9270908827366285 0.9750554323725056\n",
      "196 tensor(0.2252, grad_fn=<NllLossBackward>) 0.9300344994250096 0.9269459341933614 0.9750554323725056\n",
      "197 tensor(0.2244, grad_fn=<NllLossBackward>) 0.9300844985916902 0.9266560371068271 0.9750554323725056\n",
      "198 tensor(0.2236, grad_fn=<NllLossBackward>) 0.9300511658139031 0.9266560371068271 0.9750554323725056\n",
      "199 tensor(0.2229, grad_fn=<NllLossBackward>) 0.9301011649805837 0.9265110885635599 0.9750554323725056\n",
      "200 tensor(0.2222, grad_fn=<NllLossBackward>) 0.9301678305361577 0.9265110885635599 0.9750554323725056\n",
      "201 tensor(0.2215, grad_fn=<NllLossBackward>) 0.9301678305361577 0.9265110885635599 0.9750554323725056\n",
      "202 tensor(0.2208, grad_fn=<NllLossBackward>) 0.9302011633139448 0.9266560371068271 0.9745011086474501\n",
      "203 tensor(0.2201, grad_fn=<NllLossBackward>) 0.9302178297028383 0.9266560371068271 0.9739467849223947\n",
      "204 tensor(0.2195, grad_fn=<NllLossBackward>) 0.9301344977583706 0.9265110885635599 0.9739467849223947\n",
      "205 tensor(0.2188, grad_fn=<NllLossBackward>) 0.9301511641472642 0.9265110885635599 0.9739467849223947\n",
      "206 tensor(0.2182, grad_fn=<NllLossBackward>) 0.9302344960917318 0.9265110885635599 0.9739467849223947\n",
      "207 tensor(0.2176, grad_fn=<NllLossBackward>) 0.9302511624806253 0.9263661400202928 0.9739467849223947\n",
      "208 tensor(0.2170, grad_fn=<NllLossBackward>) 0.9302678288695189 0.9262211914770256 0.9739467849223947\n",
      "209 tensor(0.2165, grad_fn=<NllLossBackward>) 0.9302678288695189 0.9265110885635599 0.9733924611973392\n",
      "210 tensor(0.2159, grad_fn=<NllLossBackward>) 0.9303344944250929 0.9265110885635599 0.9733924611973392\n",
      "211 tensor(0.2153, grad_fn=<NllLossBackward>) 0.9304178263695605 0.9265110885635599 0.9733924611973392\n",
      "212 tensor(0.2148, grad_fn=<NllLossBackward>) 0.9304511591473476 0.9265110885635599 0.9733924611973392\n",
      "213 tensor(0.2143, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9265110885635599 0.9733924611973392\n",
      "214 tensor(0.2138, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9265110885635599 0.9733924611973392\n",
      "215 tensor(0.2133, grad_fn=<NllLossBackward>) 0.9305178247029217 0.9265110885635599 0.9739467849223947\n",
      "216 tensor(0.2128, grad_fn=<NllLossBackward>) 0.9304678255362411 0.9265110885635599 0.9733924611973392\n",
      "217 tensor(0.2123, grad_fn=<NllLossBackward>) 0.9305178247029217 0.9265110885635599 0.9739467849223947\n",
      "218 tensor(0.2119, grad_fn=<NllLossBackward>) 0.9304844919251346 0.9265110885635599 0.9739467849223947\n",
      "219 tensor(0.2114, grad_fn=<NllLossBackward>) 0.9305344910918152 0.9265110885635599 0.9739467849223947\n",
      "220 tensor(0.2110, grad_fn=<NllLossBackward>) 0.9305511574807086 0.9263661400202928 0.9739467849223947\n",
      "221 tensor(0.2105, grad_fn=<NllLossBackward>) 0.9305511574807086 0.9265110885635599 0.9733924611973392\n",
      "222 tensor(0.2101, grad_fn=<NllLossBackward>) 0.9305844902584957 0.9265110885635599 0.9733924611973392\n",
      "223 tensor(0.2097, grad_fn=<NllLossBackward>) 0.9306344894251762 0.9263661400202928 0.9739467849223947\n",
      "224 tensor(0.2093, grad_fn=<NllLossBackward>) 0.9306511558140698 0.9263661400202928 0.9739467849223947\n",
      "225 tensor(0.2089, grad_fn=<NllLossBackward>) 0.9306511558140698 0.9263661400202928 0.9739467849223947\n",
      "226 tensor(0.2085, grad_fn=<NllLossBackward>) 0.9306678222029633 0.9263661400202928 0.9739467849223947\n",
      "227 tensor(0.2081, grad_fn=<NllLossBackward>) 0.9307344877585374 0.9262211914770256 0.9739467849223947\n",
      "228 tensor(0.2078, grad_fn=<NllLossBackward>) 0.9307678205363243 0.9263661400202928 0.9739467849223947\n",
      "229 tensor(0.2074, grad_fn=<NllLossBackward>) 0.9307678205363243 0.9263661400202928 0.9739467849223947\n",
      "230 tensor(0.2071, grad_fn=<NllLossBackward>) 0.9308011533141114 0.9263661400202928 0.9739467849223947\n",
      "231 tensor(0.2067, grad_fn=<NllLossBackward>) 0.9307844869252179 0.9266560371068271 0.9739467849223947\n",
      "232 tensor(0.2064, grad_fn=<NllLossBackward>) 0.9307844869252179 0.9266560371068271 0.9739467849223947\n",
      "233 tensor(0.2060, grad_fn=<NllLossBackward>) 0.9307844869252179 0.9266560371068271 0.9739467849223947\n",
      "234 tensor(0.2057, grad_fn=<NllLossBackward>) 0.9308178197030049 0.9268009856500942 0.9739467849223947\n",
      "235 tensor(0.2054, grad_fn=<NllLossBackward>) 0.930851152480792 0.9268009856500942 0.9739467849223947\n",
      "236 tensor(0.2051, grad_fn=<NllLossBackward>) 0.9308678188696855 0.9268009856500942 0.9739467849223947\n",
      "237 tensor(0.2048, grad_fn=<NllLossBackward>) 0.930884485258579 0.9266560371068271 0.9739467849223947\n",
      "238 tensor(0.2045, grad_fn=<NllLossBackward>) 0.930851152480792 0.9266560371068271 0.9739467849223947\n",
      "239 tensor(0.2042, grad_fn=<NllLossBackward>) 0.930884485258579 0.9265110885635599 0.9739467849223947\n",
      "240 tensor(0.2039, grad_fn=<NllLossBackward>) 0.9308678188696855 0.9265110885635599 0.9739467849223947\n",
      "241 tensor(0.2036, grad_fn=<NllLossBackward>) 0.9308678188696855 0.9265110885635599 0.9745011086474501\n",
      "242 tensor(0.2034, grad_fn=<NllLossBackward>) 0.9308344860918984 0.9265110885635599 0.9745011086474501\n",
      "243 tensor(0.2031, grad_fn=<NllLossBackward>) 0.9308178197030049 0.9265110885635599 0.9745011086474501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244 tensor(0.2029, grad_fn=<NllLossBackward>) 0.930851152480792 0.9265110885635599 0.9745011086474501\n",
      "245 tensor(0.2026, grad_fn=<NllLossBackward>) 0.930851152480792 0.9265110885635599 0.9745011086474501\n",
      "246 tensor(0.2023, grad_fn=<NllLossBackward>) 0.9308678188696855 0.9265110885635599 0.9745011086474501\n",
      "247 tensor(0.2021, grad_fn=<NllLossBackward>) 0.9309011516474726 0.9265110885635599 0.9745011086474501\n",
      "248 tensor(0.2019, grad_fn=<NllLossBackward>) 0.9309011516474726 0.9265110885635599 0.9745011086474501\n",
      "249 tensor(0.2016, grad_fn=<NllLossBackward>) 0.9309178180363661 0.9266560371068271 0.9745011086474501\n",
      "250 tensor(0.2014, grad_fn=<NllLossBackward>) 0.9309511508141531 0.9266560371068271 0.9745011086474501\n",
      "251 tensor(0.2012, grad_fn=<NllLossBackward>) 0.9309678172030467 0.9266560371068271 0.9750554323725056\n",
      "252 tensor(0.2009, grad_fn=<NllLossBackward>) 0.9310344827586207 0.9266560371068271 0.9750554323725056\n",
      "253 tensor(0.2007, grad_fn=<NllLossBackward>) 0.9310511491475142 0.9266560371068271 0.9750554323725056\n",
      "254 tensor(0.2005, grad_fn=<NllLossBackward>) 0.9310678155364077 0.9265110885635599 0.9750554323725056\n",
      "255 tensor(0.2003, grad_fn=<NllLossBackward>) 0.9311178147030883 0.9266560371068271 0.9750554323725056\n",
      "256 tensor(0.2001, grad_fn=<NllLossBackward>) 0.9311511474808754 0.9266560371068271 0.9750554323725056\n",
      "257 tensor(0.1999, grad_fn=<NllLossBackward>) 0.9311844802586624 0.9266560371068271 0.9750554323725056\n",
      "258 tensor(0.1997, grad_fn=<NllLossBackward>) 0.9311844802586624 0.9266560371068271 0.9750554323725056\n",
      "259 tensor(0.1995, grad_fn=<NllLossBackward>) 0.9311844802586624 0.9266560371068271 0.9750554323725056\n",
      "260 tensor(0.1993, grad_fn=<NllLossBackward>) 0.9312344794253429 0.9266560371068271 0.9750554323725056\n",
      "261 tensor(0.1991, grad_fn=<NllLossBackward>) 0.931301144980917 0.9266560371068271 0.975609756097561\n",
      "262 tensor(0.1989, grad_fn=<NllLossBackward>) 0.9312844785920235 0.9266560371068271 0.975609756097561\n",
      "263 tensor(0.1987, grad_fn=<NllLossBackward>) 0.931301144980917 0.9266560371068271 0.975609756097561\n",
      "264 tensor(0.1985, grad_fn=<NllLossBackward>) 0.931334477758704 0.9266560371068271 0.975609756097561\n",
      "265 tensor(0.1984, grad_fn=<NllLossBackward>) 0.9312678122031299 0.9266560371068271 0.975609756097561\n",
      "266 tensor(0.1982, grad_fn=<NllLossBackward>) 0.931301144980917 0.9266560371068271 0.975609756097561\n",
      "267 tensor(0.1980, grad_fn=<NllLossBackward>) 0.9312844785920235 0.9266560371068271 0.975609756097561\n",
      "268 tensor(0.1979, grad_fn=<NllLossBackward>) 0.9312844785920235 0.9266560371068271 0.975609756097561\n",
      "269 tensor(0.1977, grad_fn=<NllLossBackward>) 0.9312844785920235 0.9266560371068271 0.975609756097561\n",
      "270 tensor(0.1975, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9266560371068271 0.975609756097561\n",
      "271 tensor(0.1974, grad_fn=<NllLossBackward>) 0.931301144980917 0.9266560371068271 0.975609756097561\n",
      "272 tensor(0.1972, grad_fn=<NllLossBackward>) 0.931301144980917 0.9266560371068271 0.975609756097561\n",
      "273 tensor(0.1970, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9266560371068271 0.975609756097561\n",
      "274 tensor(0.1969, grad_fn=<NllLossBackward>) 0.931301144980917 0.9268009856500942 0.975609756097561\n",
      "275 tensor(0.1967, grad_fn=<NllLossBackward>) 0.9312844785920235 0.9268009856500942 0.975609756097561\n",
      "276 tensor(0.1966, grad_fn=<NllLossBackward>) 0.931301144980917 0.9268009856500942 0.975609756097561\n",
      "277 tensor(0.1964, grad_fn=<NllLossBackward>) 0.931301144980917 0.9268009856500942 0.975609756097561\n",
      "278 tensor(0.1963, grad_fn=<NllLossBackward>) 0.931334477758704 0.9268009856500942 0.975609756097561\n",
      "279 tensor(0.1962, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9268009856500942 0.975609756097561\n",
      "280 tensor(0.1960, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9266560371068271 0.975609756097561\n",
      "281 tensor(0.1959, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9266560371068271 0.975609756097561\n",
      "282 tensor(0.1957, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9266560371068271 0.975609756097561\n",
      "283 tensor(0.1956, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9266560371068271 0.975609756097561\n",
      "284 tensor(0.1955, grad_fn=<NllLossBackward>) 0.931334477758704 0.9266560371068271 0.975609756097561\n",
      "285 tensor(0.1953, grad_fn=<NllLossBackward>) 0.931334477758704 0.9266560371068271 0.975609756097561\n",
      "286 tensor(0.1952, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9268009856500942 0.975609756097561\n",
      "287 tensor(0.1951, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9269459341933614 0.975609756097561\n",
      "288 tensor(0.1950, grad_fn=<NllLossBackward>) 0.931301144980917 0.9269459341933614 0.975609756097561\n",
      "289 tensor(0.1948, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9269459341933614 0.9761640798226164\n",
      "290 tensor(0.1947, grad_fn=<NllLossBackward>) 0.931334477758704 0.9269459341933614 0.9767184035476718\n",
      "291 tensor(0.1946, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9269459341933614 0.9767184035476718\n",
      "292 tensor(0.1945, grad_fn=<NllLossBackward>) 0.9313178113698105 0.9270908827366285 0.9767184035476718\n",
      "293 tensor(0.1944, grad_fn=<NllLossBackward>) 0.931334477758704 0.9270908827366285 0.9767184035476718\n",
      "294 tensor(0.1942, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9270908827366285 0.9767184035476718\n",
      "295 tensor(0.1941, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9270908827366285 0.9767184035476718\n",
      "296 tensor(0.1940, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9270908827366285 0.9767184035476718\n",
      "297 tensor(0.1939, grad_fn=<NllLossBackward>) 0.9313511441475976 0.9270908827366285 0.9767184035476718\n",
      "298 tensor(0.1938, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9272358312798956 0.9767184035476718\n",
      "299 tensor(0.1937, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9272358312798956 0.9767184035476718\n",
      "300 tensor(0.1936, grad_fn=<NllLossBackward>) 0.9314011433142781 0.9272358312798956 0.9772727272727273\n",
      "301 tensor(0.1935, grad_fn=<NllLossBackward>) 0.9314011433142781 0.9272358312798956 0.9772727272727273\n",
      "302 tensor(0.1934, grad_fn=<NllLossBackward>) 0.9313844769253846 0.9272358312798956 0.9772727272727273\n",
      "303 tensor(0.1932, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9273807798231628 0.9772727272727273\n",
      "304 tensor(0.1931, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9273807798231628 0.9772727272727273\n",
      "305 tensor(0.1930, grad_fn=<NllLossBackward>) 0.9313678105364911 0.9273807798231628 0.9772727272727273\n",
      "306 tensor(0.1929, grad_fn=<NllLossBackward>) 0.9314178097031716 0.9275257283664299 0.9772727272727273\n",
      "307 tensor(0.1928, grad_fn=<NllLossBackward>) 0.9314344760920651 0.9275257283664299 0.9772727272727273\n",
      "308 tensor(0.1927, grad_fn=<NllLossBackward>) 0.9314344760920651 0.9275257283664299 0.9772727272727273\n",
      "309 tensor(0.1926, grad_fn=<NllLossBackward>) 0.9314511424809586 0.9275257283664299 0.9772727272727273\n",
      "310 tensor(0.1925, grad_fn=<NllLossBackward>) 0.9314511424809586 0.9276706769096971 0.9772727272727273\n",
      "311 tensor(0.1924, grad_fn=<NllLossBackward>) 0.9314511424809586 0.9276706769096971 0.9772727272727273\n",
      "312 tensor(0.1923, grad_fn=<NllLossBackward>) 0.9314678088698521 0.9276706769096971 0.9772727272727273\n",
      "313 tensor(0.1923, grad_fn=<NllLossBackward>) 0.9314678088698521 0.9276706769096971 0.9772727272727273\n",
      "314 tensor(0.1922, grad_fn=<NllLossBackward>) 0.9314844752587457 0.9276706769096971 0.9772727272727273\n",
      "315 tensor(0.1921, grad_fn=<NllLossBackward>) 0.9315178080365327 0.9278156254529641 0.9772727272727273\n",
      "316 tensor(0.1920, grad_fn=<NllLossBackward>) 0.9315344744254263 0.9278156254529641 0.9772727272727273\n",
      "317 tensor(0.1919, grad_fn=<NllLossBackward>) 0.9315344744254263 0.9278156254529641 0.9772727272727273\n",
      "318 tensor(0.1918, grad_fn=<NllLossBackward>) 0.9315678072032133 0.9278156254529641 0.9772727272727273\n",
      "319 tensor(0.1917, grad_fn=<NllLossBackward>) 0.9316011399810004 0.9278156254529641 0.9772727272727273\n",
      "320 tensor(0.1916, grad_fn=<NllLossBackward>) 0.9316178063698939 0.9278156254529641 0.9772727272727273\n",
      "321 tensor(0.1915, grad_fn=<NllLossBackward>) 0.9316678055365744 0.9278156254529641 0.9772727272727273\n",
      "322 tensor(0.1914, grad_fn=<NllLossBackward>) 0.9317011383143614 0.9279605739962313 0.9772727272727273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323 tensor(0.1914, grad_fn=<NllLossBackward>) 0.9316844719254679 0.9279605739962313 0.9772727272727273\n",
      "324 tensor(0.1913, grad_fn=<NllLossBackward>) 0.9316678055365744 0.9279605739962313 0.9772727272727273\n",
      "325 tensor(0.1912, grad_fn=<NllLossBackward>) 0.9316511391476808 0.9281055225394985 0.9772727272727273\n",
      "326 tensor(0.1911, grad_fn=<NllLossBackward>) 0.9316511391476808 0.9281055225394985 0.9772727272727273\n",
      "327 tensor(0.1910, grad_fn=<NllLossBackward>) 0.9316844719254679 0.9279605739962313 0.9772727272727273\n",
      "328 tensor(0.1909, grad_fn=<NllLossBackward>) 0.9317011383143614 0.9279605739962313 0.9772727272727273\n",
      "329 tensor(0.1908, grad_fn=<NllLossBackward>) 0.9316844719254679 0.9279605739962313 0.9772727272727273\n",
      "330 tensor(0.1908, grad_fn=<NllLossBackward>) 0.9316678055365744 0.9279605739962313 0.9772727272727273\n",
      "331 tensor(0.1907, grad_fn=<NllLossBackward>) 0.9316678055365744 0.9281055225394985 0.9772727272727273\n",
      "332 tensor(0.1906, grad_fn=<NllLossBackward>) 0.9316511391476808 0.9281055225394985 0.9772727272727273\n",
      "333 tensor(0.1905, grad_fn=<NllLossBackward>) 0.9317011383143614 0.9281055225394985 0.9772727272727273\n",
      "334 tensor(0.1904, grad_fn=<NllLossBackward>) 0.931751137481042 0.9281055225394985 0.9772727272727273\n",
      "335 tensor(0.1904, grad_fn=<NllLossBackward>) 0.931751137481042 0.9281055225394985 0.9772727272727273\n",
      "336 tensor(0.1903, grad_fn=<NllLossBackward>) 0.931751137481042 0.9281055225394985 0.9772727272727273\n",
      "337 tensor(0.1902, grad_fn=<NllLossBackward>) 0.931751137481042 0.9281055225394985 0.9772727272727273\n",
      "338 tensor(0.1901, grad_fn=<NllLossBackward>) 0.931784470258829 0.9282504710827656 0.9772727272727273\n",
      "339 tensor(0.1901, grad_fn=<NllLossBackward>) 0.931784470258829 0.9282504710827656 0.9772727272727273\n",
      "340 tensor(0.1900, grad_fn=<NllLossBackward>) 0.9318011366477226 0.9282504710827656 0.9772727272727273\n",
      "341 tensor(0.1899, grad_fn=<NllLossBackward>) 0.9318344694255096 0.9282504710827656 0.9772727272727273\n",
      "342 tensor(0.1898, grad_fn=<NllLossBackward>) 0.931851135814403 0.9282504710827656 0.9772727272727273\n",
      "343 tensor(0.1897, grad_fn=<NllLossBackward>) 0.9318844685921901 0.9282504710827656 0.9772727272727273\n",
      "344 tensor(0.1897, grad_fn=<NllLossBackward>) 0.9318844685921901 0.9282504710827656 0.9772727272727273\n",
      "345 tensor(0.1896, grad_fn=<NllLossBackward>) 0.9319011349810836 0.9282504710827656 0.9772727272727273\n",
      "346 tensor(0.1895, grad_fn=<NllLossBackward>) 0.9319011349810836 0.9282504710827656 0.9772727272727273\n",
      "347 tensor(0.1895, grad_fn=<NllLossBackward>) 0.9319011349810836 0.9282504710827656 0.9772727272727273\n",
      "348 tensor(0.1894, grad_fn=<NllLossBackward>) 0.9319178013699772 0.9282504710827656 0.9772727272727273\n",
      "349 tensor(0.1893, grad_fn=<NllLossBackward>) 0.9319178013699772 0.9282504710827656 0.9772727272727273\n",
      "350 tensor(0.1892, grad_fn=<NllLossBackward>) 0.9319511341477642 0.9282504710827656 0.9772727272727273\n",
      "351 tensor(0.1892, grad_fn=<NllLossBackward>) 0.9319844669255513 0.9282504710827656 0.9772727272727273\n",
      "352 tensor(0.1891, grad_fn=<NllLossBackward>) 0.9319844669255513 0.9282504710827656 0.9772727272727273\n",
      "353 tensor(0.1890, grad_fn=<NllLossBackward>) 0.9320011333144448 0.9283954196260328 0.9772727272727273\n",
      "354 tensor(0.1890, grad_fn=<NllLossBackward>) 0.9320011333144448 0.9283954196260328 0.9772727272727273\n",
      "355 tensor(0.1889, grad_fn=<NllLossBackward>) 0.9320011333144448 0.9283954196260328 0.9772727272727273\n",
      "356 tensor(0.1888, grad_fn=<NllLossBackward>) 0.9320177997033383 0.9283954196260328 0.9772727272727273\n",
      "357 tensor(0.1887, grad_fn=<NllLossBackward>) 0.9320177997033383 0.9282504710827656 0.9772727272727273\n",
      "358 tensor(0.1887, grad_fn=<NllLossBackward>) 0.9320177997033383 0.9283954196260328 0.9772727272727273\n",
      "359 tensor(0.1886, grad_fn=<NllLossBackward>) 0.9320511324811254 0.9283954196260328 0.9772727272727273\n",
      "360 tensor(0.1885, grad_fn=<NllLossBackward>) 0.9320677988700188 0.9283954196260328 0.9772727272727273\n",
      "361 tensor(0.1885, grad_fn=<NllLossBackward>) 0.9320677988700188 0.9283954196260328 0.9772727272727273\n",
      "362 tensor(0.1884, grad_fn=<NllLossBackward>) 0.9321011316478058 0.9283954196260328 0.9772727272727273\n",
      "363 tensor(0.1883, grad_fn=<NllLossBackward>) 0.9321177980366994 0.9283954196260328 0.9772727272727273\n",
      "364 tensor(0.1883, grad_fn=<NllLossBackward>) 0.9321511308144864 0.9283954196260328 0.9772727272727273\n",
      "365 tensor(0.1882, grad_fn=<NllLossBackward>) 0.9321677972033799 0.9285403681692999 0.9772727272727273\n",
      "366 tensor(0.1881, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9285403681692999 0.9767184035476718\n",
      "367 tensor(0.1881, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9285403681692999 0.9767184035476718\n",
      "368 tensor(0.1880, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9285403681692999 0.9767184035476718\n",
      "369 tensor(0.1879, grad_fn=<NllLossBackward>) 0.932201129981167 0.9285403681692999 0.9767184035476718\n",
      "370 tensor(0.1879, grad_fn=<NllLossBackward>) 0.932201129981167 0.9285403681692999 0.9767184035476718\n",
      "371 tensor(0.1878, grad_fn=<NllLossBackward>) 0.9321844635922735 0.9285403681692999 0.9761640798226164\n",
      "372 tensor(0.1878, grad_fn=<NllLossBackward>) 0.932201129981167 0.9285403681692999 0.9761640798226164\n",
      "373 tensor(0.1877, grad_fn=<NllLossBackward>) 0.932234462758954 0.9285403681692999 0.9761640798226164\n",
      "374 tensor(0.1876, grad_fn=<NllLossBackward>) 0.9322677955367411 0.9285403681692999 0.9761640798226164\n",
      "375 tensor(0.1876, grad_fn=<NllLossBackward>) 0.9322677955367411 0.9285403681692999 0.975609756097561\n",
      "376 tensor(0.1875, grad_fn=<NllLossBackward>) 0.9322677955367411 0.9285403681692999 0.975609756097561\n",
      "377 tensor(0.1874, grad_fn=<NllLossBackward>) 0.9322677955367411 0.9285403681692999 0.975609756097561\n",
      "378 tensor(0.1874, grad_fn=<NllLossBackward>) 0.9322677955367411 0.928685316712567 0.975609756097561\n",
      "379 tensor(0.1873, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9288302652558342 0.975609756097561\n",
      "380 tensor(0.1873, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9288302652558342 0.975609756097561\n",
      "381 tensor(0.1872, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9289752137991013 0.975609756097561\n",
      "382 tensor(0.1871, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9288302652558342 0.975609756097561\n",
      "383 tensor(0.1871, grad_fn=<NllLossBackward>) 0.9322677955367411 0.9288302652558342 0.975609756097561\n",
      "384 tensor(0.1870, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9288302652558342 0.975609756097561\n",
      "385 tensor(0.1869, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9289752137991013 0.975609756097561\n",
      "386 tensor(0.1869, grad_fn=<NllLossBackward>) 0.9322844619256345 0.9289752137991013 0.975609756097561\n",
      "387 tensor(0.1868, grad_fn=<NllLossBackward>) 0.932301128314528 0.9291201623423685 0.975609756097561\n",
      "388 tensor(0.1868, grad_fn=<NllLossBackward>) 0.9323344610923151 0.9291201623423685 0.975609756097561\n",
      "389 tensor(0.1867, grad_fn=<NllLossBackward>) 0.9323511274812086 0.9291201623423685 0.975609756097561\n",
      "390 tensor(0.1866, grad_fn=<NllLossBackward>) 0.9324011266478892 0.9291201623423685 0.975609756097561\n",
      "391 tensor(0.1866, grad_fn=<NllLossBackward>) 0.9324011266478892 0.9292651108856356 0.975609756097561\n",
      "392 tensor(0.1865, grad_fn=<NllLossBackward>) 0.9323677938701022 0.9294100594289028 0.975609756097561\n",
      "393 tensor(0.1865, grad_fn=<NllLossBackward>) 0.9323677938701022 0.9294100594289028 0.975609756097561\n",
      "394 tensor(0.1864, grad_fn=<NllLossBackward>) 0.9324011266478892 0.9294100594289028 0.975609756097561\n",
      "395 tensor(0.1864, grad_fn=<NllLossBackward>) 0.9323844602589957 0.9295550079721698 0.975609756097561\n",
      "396 tensor(0.1863, grad_fn=<NllLossBackward>) 0.9323677938701022 0.9295550079721698 0.975609756097561\n",
      "397 tensor(0.1862, grad_fn=<NllLossBackward>) 0.9323844602589957 0.9295550079721698 0.975609756097561\n",
      "398 tensor(0.1862, grad_fn=<NllLossBackward>) 0.9323844602589957 0.9295550079721698 0.975609756097561\n",
      "399 tensor(0.1861, grad_fn=<NllLossBackward>) 0.9323844602589957 0.9295550079721698 0.975609756097561\n",
      "400 tensor(0.1861, grad_fn=<NllLossBackward>) 0.9323844602589957 0.9295550079721698 0.975609756097561\n",
      "401 tensor(0.1860, grad_fn=<NllLossBackward>) 0.9324011266478892 0.9295550079721698 0.975609756097561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "402 tensor(0.1860, grad_fn=<NllLossBackward>) 0.9324177930367827 0.9298449050587042 0.975609756097561\n",
      "403 tensor(0.1859, grad_fn=<NllLossBackward>) 0.9324511258145698 0.9298449050587042 0.975609756097561\n",
      "404 tensor(0.1858, grad_fn=<NllLossBackward>) 0.9324677922034633 0.9298449050587042 0.975609756097561\n",
      "405 tensor(0.1858, grad_fn=<NllLossBackward>) 0.9324844585923568 0.9298449050587042 0.975609756097561\n",
      "406 tensor(0.1857, grad_fn=<NllLossBackward>) 0.9325344577590373 0.9298449050587042 0.975609756097561\n",
      "407 tensor(0.1857, grad_fn=<NllLossBackward>) 0.9325344577590373 0.9298449050587042 0.975609756097561\n",
      "408 tensor(0.1856, grad_fn=<NllLossBackward>) 0.9325677905368244 0.9299898536019713 0.975609756097561\n",
      "409 tensor(0.1856, grad_fn=<NllLossBackward>) 0.9326011233146114 0.9299898536019713 0.975609756097561\n",
      "410 tensor(0.1855, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9299898536019713 0.975609756097561\n",
      "411 tensor(0.1854, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9299898536019713 0.975609756097561\n",
      "412 tensor(0.1854, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9299898536019713 0.975609756097561\n",
      "413 tensor(0.1853, grad_fn=<NllLossBackward>) 0.9326011233146114 0.9301348021452385 0.975609756097561\n",
      "414 tensor(0.1853, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9301348021452385 0.975609756097561\n",
      "415 tensor(0.1852, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9301348021452385 0.975609756097561\n",
      "416 tensor(0.1852, grad_fn=<NllLossBackward>) 0.9325844569257179 0.9301348021452385 0.975609756097561\n",
      "417 tensor(0.1851, grad_fn=<NllLossBackward>) 0.932651122481292 0.9301348021452385 0.975609756097561\n",
      "418 tensor(0.1851, grad_fn=<NllLossBackward>) 0.9326844552590791 0.9301348021452385 0.975609756097561\n",
      "419 tensor(0.1850, grad_fn=<NllLossBackward>) 0.9326844552590791 0.9302797506885055 0.975609756097561\n",
      "420 tensor(0.1850, grad_fn=<NllLossBackward>) 0.9327011216479726 0.9302797506885055 0.9761640798226164\n",
      "421 tensor(0.1849, grad_fn=<NllLossBackward>) 0.932717788036866 0.9302797506885055 0.9761640798226164\n",
      "422 tensor(0.1849, grad_fn=<NllLossBackward>) 0.9327844535924401 0.9302797506885055 0.9761640798226164\n",
      "423 tensor(0.1848, grad_fn=<NllLossBackward>) 0.9328177863702272 0.9302797506885055 0.9761640798226164\n",
      "424 tensor(0.1848, grad_fn=<NllLossBackward>) 0.9328177863702272 0.9304246992317727 0.9761640798226164\n",
      "425 tensor(0.1847, grad_fn=<NllLossBackward>) 0.9328511191480142 0.9304246992317727 0.9761640798226164\n",
      "426 tensor(0.1846, grad_fn=<NllLossBackward>) 0.9328844519258013 0.9305696477750398 0.9761640798226164\n",
      "427 tensor(0.1846, grad_fn=<NllLossBackward>) 0.9329011183146948 0.9305696477750398 0.9761640798226164\n",
      "428 tensor(0.1845, grad_fn=<NllLossBackward>) 0.9329344510924817 0.9305696477750398 0.9761640798226164\n",
      "429 tensor(0.1845, grad_fn=<NllLossBackward>) 0.9329844502591623 0.9305696477750398 0.9761640798226164\n",
      "430 tensor(0.1844, grad_fn=<NllLossBackward>) 0.9330011166480559 0.9305696477750398 0.9761640798226164\n",
      "431 tensor(0.1844, grad_fn=<NllLossBackward>) 0.9330177830369494 0.9305696477750398 0.9761640798226164\n",
      "432 tensor(0.1843, grad_fn=<NllLossBackward>) 0.9330177830369494 0.9305696477750398 0.9761640798226164\n",
      "433 tensor(0.1843, grad_fn=<NllLossBackward>) 0.9330511158147364 0.930714596318307 0.9761640798226164\n",
      "434 tensor(0.1842, grad_fn=<NllLossBackward>) 0.9330844485925235 0.930714596318307 0.975609756097561\n",
      "435 tensor(0.1842, grad_fn=<NllLossBackward>) 0.933101114981417 0.930714596318307 0.975609756097561\n",
      "436 tensor(0.1841, grad_fn=<NllLossBackward>) 0.9331177813703105 0.930714596318307 0.975609756097561\n",
      "437 tensor(0.1841, grad_fn=<NllLossBackward>) 0.9331344477592041 0.930714596318307 0.975609756097561\n",
      "438 tensor(0.1840, grad_fn=<NllLossBackward>) 0.9331511141480975 0.9308595448615742 0.975609756097561\n",
      "439 tensor(0.1840, grad_fn=<NllLossBackward>) 0.9331511141480975 0.9308595448615742 0.975609756097561\n",
      "440 tensor(0.1839, grad_fn=<NllLossBackward>) 0.9331844469258845 0.9310044934048413 0.975609756097561\n",
      "441 tensor(0.1839, grad_fn=<NllLossBackward>) 0.9331844469258845 0.9310044934048413 0.975609756097561\n",
      "442 tensor(0.1838, grad_fn=<NllLossBackward>) 0.9332177797036716 0.9310044934048413 0.975609756097561\n",
      "443 tensor(0.1838, grad_fn=<NllLossBackward>) 0.9332511124814586 0.9310044934048413 0.975609756097561\n",
      "444 tensor(0.1837, grad_fn=<NllLossBackward>) 0.9333011116481392 0.9310044934048413 0.975609756097561\n",
      "445 tensor(0.1837, grad_fn=<NllLossBackward>) 0.9333177780370328 0.9310044934048413 0.975609756097561\n",
      "446 tensor(0.1836, grad_fn=<NllLossBackward>) 0.9333344444259263 0.9311494419481084 0.975609756097561\n",
      "447 tensor(0.1836, grad_fn=<NllLossBackward>) 0.9333677772037132 0.9311494419481084 0.975609756097561\n",
      "448 tensor(0.1835, grad_fn=<NllLossBackward>) 0.9333511108148198 0.9311494419481084 0.975609756097561\n",
      "449 tensor(0.1835, grad_fn=<NllLossBackward>) 0.9333677772037132 0.9311494419481084 0.975609756097561\n",
      "450 tensor(0.1834, grad_fn=<NllLossBackward>) 0.9333677772037132 0.9312943904913755 0.975609756097561\n",
      "451 tensor(0.1834, grad_fn=<NllLossBackward>) 0.9334011099815003 0.9312943904913755 0.975609756097561\n",
      "452 tensor(0.1833, grad_fn=<NllLossBackward>) 0.9334011099815003 0.9312943904913755 0.975609756097561\n",
      "453 tensor(0.1833, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9312943904913755 0.975609756097561\n",
      "454 tensor(0.1832, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9311494419481084 0.9761640798226164\n",
      "455 tensor(0.1832, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9311494419481084 0.9761640798226164\n",
      "456 tensor(0.1831, grad_fn=<NllLossBackward>) 0.9334011099815003 0.9311494419481084 0.9761640798226164\n",
      "457 tensor(0.1831, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9311494419481084 0.9761640798226164\n",
      "458 tensor(0.1830, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9312943904913755 0.9761640798226164\n",
      "459 tensor(0.1830, grad_fn=<NllLossBackward>) 0.9334177763703938 0.9312943904913755 0.9761640798226164\n",
      "460 tensor(0.1829, grad_fn=<NllLossBackward>) 0.9334511091481809 0.9312943904913755 0.9761640798226164\n",
      "461 tensor(0.1829, grad_fn=<NllLossBackward>) 0.9334677755370744 0.9312943904913755 0.9761640798226164\n",
      "462 tensor(0.1828, grad_fn=<NllLossBackward>) 0.9334511091481809 0.9311494419481084 0.9761640798226164\n",
      "463 tensor(0.1828, grad_fn=<NllLossBackward>) 0.9334511091481809 0.9311494419481084 0.9761640798226164\n",
      "464 tensor(0.1828, grad_fn=<NllLossBackward>) 0.9334677755370744 0.9311494419481084 0.9761640798226164\n",
      "465 tensor(0.1827, grad_fn=<NllLossBackward>) 0.9334677755370744 0.9311494419481084 0.9761640798226164\n",
      "466 tensor(0.1827, grad_fn=<NllLossBackward>) 0.9334844419259679 0.9311494419481084 0.9761640798226164\n",
      "467 tensor(0.1826, grad_fn=<NllLossBackward>) 0.933517774703755 0.9311494419481084 0.9761640798226164\n",
      "468 tensor(0.1826, grad_fn=<NllLossBackward>) 0.933517774703755 0.9312943904913755 0.9761640798226164\n",
      "469 tensor(0.1825, grad_fn=<NllLossBackward>) 0.9335344410926485 0.9312943904913755 0.9761640798226164\n",
      "470 tensor(0.1825, grad_fn=<NllLossBackward>) 0.933584440259329 0.9312943904913755 0.9761640798226164\n",
      "471 tensor(0.1824, grad_fn=<NllLossBackward>) 0.9336011066482225 0.9312943904913755 0.9761640798226164\n",
      "472 tensor(0.1824, grad_fn=<NllLossBackward>) 0.9336344394260095 0.9312943904913755 0.9761640798226164\n",
      "473 tensor(0.1823, grad_fn=<NllLossBackward>) 0.9336344394260095 0.9314393390346427 0.9761640798226164\n",
      "474 tensor(0.1823, grad_fn=<NllLossBackward>) 0.933617773037116 0.9314393390346427 0.9761640798226164\n",
      "475 tensor(0.1822, grad_fn=<NllLossBackward>) 0.9336344394260095 0.9314393390346427 0.9761640798226164\n",
      "476 tensor(0.1822, grad_fn=<NllLossBackward>) 0.9336677722037966 0.9315842875779099 0.975609756097561\n",
      "477 tensor(0.1821, grad_fn=<NllLossBackward>) 0.9336844385926901 0.9315842875779099 0.975609756097561\n",
      "478 tensor(0.1821, grad_fn=<NllLossBackward>) 0.9337344377593707 0.9315842875779099 0.975609756097561\n",
      "479 tensor(0.1821, grad_fn=<NllLossBackward>) 0.9337177713704772 0.9315842875779099 0.975609756097561\n",
      "480 tensor(0.1820, grad_fn=<NllLossBackward>) 0.9337344377593707 0.9315842875779099 0.975609756097561\n",
      "481 tensor(0.1820, grad_fn=<NllLossBackward>) 0.9338011033149447 0.9315842875779099 0.975609756097561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 tensor(0.1819, grad_fn=<NllLossBackward>) 0.9338011033149447 0.9315842875779099 0.975609756097561\n",
      "483 tensor(0.1819, grad_fn=<NllLossBackward>) 0.9338011033149447 0.931729236121177 0.975609756097561\n",
      "484 tensor(0.1818, grad_fn=<NllLossBackward>) 0.9338177697038382 0.931729236121177 0.975609756097561\n",
      "485 tensor(0.1818, grad_fn=<NllLossBackward>) 0.9338344360927318 0.931729236121177 0.975609756097561\n",
      "486 tensor(0.1817, grad_fn=<NllLossBackward>) 0.9338511024816253 0.931729236121177 0.975609756097561\n",
      "487 tensor(0.1817, grad_fn=<NllLossBackward>) 0.9338344360927318 0.931729236121177 0.975609756097561\n",
      "488 tensor(0.1816, grad_fn=<NllLossBackward>) 0.9338511024816253 0.9315842875779099 0.975609756097561\n",
      "489 tensor(0.1816, grad_fn=<NllLossBackward>) 0.9338677688705188 0.9315842875779099 0.975609756097561\n",
      "490 tensor(0.1815, grad_fn=<NllLossBackward>) 0.9339011016483059 0.9315842875779099 0.975609756097561\n",
      "491 tensor(0.1815, grad_fn=<NllLossBackward>) 0.9339177680371994 0.9315842875779099 0.975609756097561\n",
      "492 tensor(0.1815, grad_fn=<NllLossBackward>) 0.9339344344260929 0.9315842875779099 0.975609756097561\n",
      "493 tensor(0.1814, grad_fn=<NllLossBackward>) 0.93396776720388 0.9315842875779099 0.975609756097561\n",
      "494 tensor(0.1814, grad_fn=<NllLossBackward>) 0.934001099981667 0.9315842875779099 0.975609756097561\n",
      "495 tensor(0.1813, grad_fn=<NllLossBackward>) 0.9339844335927735 0.9315842875779099 0.975609756097561\n",
      "496 tensor(0.1813, grad_fn=<NllLossBackward>) 0.9340177663705604 0.9315842875779099 0.975609756097561\n",
      "497 tensor(0.1812, grad_fn=<NllLossBackward>) 0.9340177663705604 0.9314393390346427 0.975609756097561\n",
      "498 tensor(0.1812, grad_fn=<NllLossBackward>) 0.934001099981667 0.9314393390346427 0.975609756097561\n",
      "499 tensor(0.1811, grad_fn=<NllLossBackward>) 0.934001099981667 0.9314393390346427 0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "#开始训练\n",
    "#并在每100轮输出结果（这里最好每轮都记录我们的损失和精度，需要改进代码）\n",
    "losslist = []\n",
    "speficlist = []\n",
    "spefictestlist = []\n",
    "spefictrainlist = []\n",
    "for i in range(500):\n",
    "    out = net(input_train)\n",
    "    loss = loss_func(out, label_train)\n",
    "    if(i%1 == 0):\n",
    "        #print(loss)\n",
    "        losslist.append(loss)\n",
    "        out_test = net(input_valid)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_test, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_valid.data.numpy()\n",
    "        speficvalid = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        speficlist.append(speficvalid)\n",
    "        #plt.figure('Draw')\n",
    "\n",
    "        out_train = net(input_train)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_train, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_train.data.numpy()\n",
    "        spefictrain = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        spefictrainlist.append(spefictrain)\n",
    "        \n",
    "        out_test = net(input_test)\n",
    "        # out是一个计算矩阵\n",
    "        prediction = torch.max(out_test, 1)[1]\n",
    "        pred_y = prediction.numpy()\n",
    "        # 预测y输出数列\n",
    "        target_y = label_test.data.numpy()\n",
    "        spefic = np.sum(pred_y == target_y)/(np.sum(pred_y == target_y) +np.sum(pred_y != target_y) )\n",
    "        spefictestlist.append(spefic)\n",
    "        \n",
    "        print(i,loss,spefictrain,speficvalid,spefic)\n",
    "        #plt.plot(speficlist)  # plot绘制折线图\n",
    "        #plt.draw()  # 显示绘图\n",
    "        #plt.pause(2)  #显示5秒\n",
    "        #plt.close()\n",
    "    # 输出与label对比\n",
    "    optimizer.zero_grad()\n",
    "    # 初始化\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (hidden): Linear(in_features=7, out_features=10, bias=True)\n",
       "  (hidden2): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (out): Linear(in_features=10, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = net.hidden.weight.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 45, -25],\n",
       "        [-49,  10],\n",
       "        [ 11,   1],\n",
       "        [  3, -35],\n",
       "        [ -5, -10],\n",
       "        [ 12, -13],\n",
       "        [ -6, -10],\n",
       "        [ 27,  25],\n",
       "        [ 18, -24],\n",
       "        [-12,  19]], dtype=torch.int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = net.out.weight.t()*100\n",
    "h1 = h1.int()\n",
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-7, -2], dtype=torch.int32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = net.out.bias*100\n",
    "b = b.int()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 从这一步开始，不再使用torch框架 \n",
    "#这里将每层的参数提取出来\n",
    "#并直接计算作为比较\n",
    "hw = net.hidden.weight.t()*1024\n",
    "hw = hw.int()\n",
    "hb = net.hidden.bias*1024\n",
    "hb = hb.int()\n",
    "hw2 = net.hidden2.weight.t()*1024\n",
    "hw2 = hw2.int()\n",
    "hb2 = net.hidden2.bias*1024\n",
    "hb2 = hb2.int()\n",
    "hw3 = net.hidden3.weight.t()*1024\n",
    "hw3 = hw3.int()\n",
    "hb3 = net.hidden3.bias*1024\n",
    "hb3 = hb3.int()\n",
    "hw4 = net.hidden4.weight.t()*1024\n",
    "hw4 = hw4.int()\n",
    "hb4 = net.hidden4.bias*1024\n",
    "hb4 = hb4.int()\n",
    "ow = net.out.weight.t()*1024\n",
    "ow = ow.int()\n",
    "ob = net.out.bias*1024\n",
    "ob = ob.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-78, -21])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. \n",
    "#将数据转换入为numpy,方便计算\n",
    "w = hw.numpy()\n",
    "hb = hb.numpy()\n",
    "hw2 = hw2.numpy()\n",
    "hb2 = hb2.numpy()\n",
    "hw3 = hw3.numpy()\n",
    "hb3 = hb3.numpy()\n",
    "hw4 = hw4.numpy()\n",
    "hb4 = hb4.numpy()\n",
    "ow = ow.numpy()\n",
    "ob = ob.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. \n",
    "#这里讲数据数据不用框架，进行检查\n",
    "dataint = data\n",
    "dataint = dataint*1024\n",
    "dataint = dataint.astype(int)\n",
    "input_trainint = dataint[0:60001]\n",
    "label_trainint = label[0:60001]\n",
    "\n",
    "input_validint = dataint[60001:]\n",
    "label_validint = label[60001:]\n",
    "\n",
    "testdataint = testdata\n",
    "testdataint = testdataint*1024\n",
    "testdataint = testdataint.astype(int)\n",
    "input_testint = testdataint\n",
    "label_testint = testlable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9375010416493058"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(input_trainint)):\n",
    "    A = input_trainint[i]\n",
    "    B = np.dot(A,hw)//1024 + hb\n",
    "    B = np.maximum(B, 0)\n",
    "    C = np.dot(B,hw2)//1024 + hb2\n",
    "    C = np.maximum(C, 0)\n",
    "    D = np.dot(C,hw3)//1024 + hb3\n",
    "    D = np.maximum(D, 0)\n",
    "    E = np.dot(D,hw4)//1024 + hb4\n",
    "    E = np.maximum(E, 0)\n",
    "    F = np.dot(E,ow)//1024 + ob\n",
    "    if F[0] > F[1]:\n",
    "        k = 0\n",
    "    else:\n",
    "        k = 1\n",
    "    if k == label_trainint[i]:\n",
    "        count = count + 1\n",
    "spefic = count / len(input_trainint)\n",
    "spefic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9373822293085955"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(input_validint)):\n",
    "    A = input_validint[i]\n",
    "    B = np.dot(A,hw)//1024 + hb\n",
    "    B = np.maximum(B, 0)\n",
    "    C = np.dot(B,hw2)//1024 + hb2\n",
    "    C = np.maximum(C, 0)\n",
    "    D = np.dot(C,hw3)//1024 + hb3\n",
    "    D = np.maximum(D, 0)\n",
    "    E = np.dot(D,hw4)//1024 + hb4\n",
    "    E = np.maximum(E, 0)\n",
    "    F = np.dot(E,ow)//1024 + ob\n",
    "    if F[0] > F[1]:\n",
    "        k = 0\n",
    "    else:\n",
    "        k = 1\n",
    "    if k == label_validint[i]:\n",
    "        count = count + 1\n",
    "spefic = count / len(input_validint)\n",
    "spefic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9805986696230599"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(input_testint)):\n",
    "    A = input_testint[i]\n",
    "    B = np.dot(A,hw)//1024 + hb\n",
    "    B = np.maximum(B, 0)\n",
    "    C = np.dot(B,hw2)//1024 + hb2\n",
    "    C = np.maximum(C, 0)\n",
    "    D = np.dot(C,hw3)//1024 + hb3\n",
    "    D = np.maximum(D, 0)\n",
    "    E = np.dot(D,hw4)//1024 + hb4\n",
    "    E = np.maximum(E, 0)\n",
    "    F = np.dot(E,ow)//1024 + ob\n",
    "    if F[0] > F[1]:\n",
    "        k = 0\n",
    "    else:\n",
    "        k = 1\n",
    "    if k == label_testint[i]:\n",
    "        count = count + 1\n",
    "spefic = count / len(input_testint)\n",
    "spefic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 17653, -16923,  11225,  -7682,  12983,  32698,  18860,  -8597,\n",
       "        17245,  -9692])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.dot(A,hw)//10000 + hb\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17653,     0, 11225,     0, 12983, 32698, 18860,     0, 17245,\n",
       "           0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(B, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -19,  -26,  468,  452,  284,  267,  392,  258,   44,  199],\n",
       "       [-266, -114,  301,   77,  160,   51, -112, -232, -162,   -7],\n",
       "       [-239,  147,  -10,  242,   52,  420,  385,  229,  102,   91],\n",
       "       [  42,  -45, -204,  -82,  304,   49, -235, -205,   24, -132],\n",
       "       [ 526, -142, -561, -371,  366, -440, -366, -531,  135,  273],\n",
       "       [  93, -323,  284,  334,  285,  275,  283,   89,   21,  474],\n",
       "       [ 474,  274, -314, -326,   86,  -16, -436, -311,  408,  495],\n",
       "       [ 198, -163, -216,   23,   88,  222,  -61,  221,  126,  -95],\n",
       "       [ 312,   90, -330,   -6,  258, -418,  283,  152,  433,  535],\n",
       "       [-199,  -33,  171,  306,  240,  -83,   36, -172, -192,  -32]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_valid = net(input_valid)\n",
    "# out是一个计算矩阵\n",
    "prediction = torch.max(out_valid, 1)[1]\n",
    "pred_valid_y = prediction.numpy()\n",
    "# 预测y输出数列\n",
    "target_valid_y = label_valid.data.numpy()\n",
    "# 实际y输出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure('Draw')\n",
    "plt.plot(speficlist[0:3])  # plot绘制折线图\n",
    "plt.draw()  # 显示绘图\n",
    "plt.pause(2)  #显示5秒\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speficlist[249]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9831024930747922"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spefic = np.sum(pred_valid_y == target_valid_y)/(np.sum(pred_valid_y == target_valid_y) +np.sum(pred_valid_y != target_valid_y) )\n",
    "spefic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CC_Never_Give_Up\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(net, '5net.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.71619268,  0.07673379,  4.19551785,  0.97972414,  0.956     ,\n",
       "        1.004     ,  1.02040816])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-272,   14,  436,  105,  102,  108,   94])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk = data[500]*100\n",
    "kk =kk.astype(int)\n",
    "kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hw = net.hidden.weight\n",
    "Hw = Hw.t()\n",
    "Hb = net.hidden.bias\n",
    "Ow = net.out.weight\n",
    "Ow = Ow.t()\n",
    "Ob = net.out.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.hidden2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = input[1000]\n",
    "hidden = torch.matmul(d,Hw)+Hb\n",
    "res = torch.matmul(hidden,Ow) + Ob\n",
    "res\n",
    "#######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
